{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras_cv import bounding_box\n",
    "import os\n",
    "import resource\n",
    "from keras_cv import visualization\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "import time\n",
    "import pickle\n",
    "import scipy.interpolate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867137e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keras_cv.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 200\n",
    "CHECKPOINT_PATH = os.getenv(\"CHECKPOINT_PATH\", \"checkpoint/\")\n",
    "INFERENCE_CHECKPOINT_PATH = os.getenv(\"INFERENCE_CHECKPOINT_PATH\", CHECKPOINT_PATH)\n",
    "\n",
    "low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c5dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_dic = {0:\"antralfollicle\",\n",
    "                   1:\"antralfolliclewn\",\n",
    "                   2:\"corpusluteum\",\n",
    "                   #3:\"negative\"\n",
    "                  }\n",
    "\n",
    "img_width = 640\n",
    "img_height = 640\n",
    "\n",
    "\"\"\"img width and size depend on the backbone architecture, for example to use the RetinaNet architecture \n",
    "with a ResNet50 backbone, we need to resize our image to a size that is divisible by 64. This is to ensure \n",
    "compatibility with the number of downscaling operations done by\n",
    "the convolution layers in the ResNet.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ac067",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c947f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"CPU\"):\n",
    "    with open('tf_data_samp/train' + '/element_spec', 'rb') as in_:\n",
    "        elemspec = pickle.load(in_)\n",
    "    train_ds = tf.data.experimental.load('tf_data_samp/train', elemspec, compression='GZIP')\n",
    "    \n",
    "    with open('tf_data_samp/val' + '/element_spec', 'rb') as in_:\n",
    "        elemspec = pickle.load(in_)\n",
    "    eval_ds = tf.data.experimental.load('tf_data_samp/val', elemspec, compression='GZIP')\n",
    "    \n",
    "    with open('tf_data_samp/test' + '/element_spec', 'rb') as in_:\n",
    "        elemspec = pickle.load(in_)\n",
    "    test_ds = tf.data.experimental.load('tf_data_samp/test', elemspec, compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To convert boundary boxes to ragged tensor format\n",
    "\n",
    "def dict_to_tuple(inputs):\n",
    "    return {\"images\": tf.RaggedTensor.from_tensor(inputs[\"images\"]), \n",
    "            \"bounding_boxes\": bounding_box.to_ragged(inputs[\"bounding_boxes\"])}\n",
    "with tf.device(\"CPU\"):\n",
    "    train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    eval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are batching our dataset.\n",
    "with tf.device(\"CPU\"):\n",
    "    train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "    eval_ds = eval_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "    test_ds = test_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21233d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we use the kerasCV function to visualize our dataset\n",
    "\n",
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_label_dic,\n",
    "    )\n",
    "\n",
    "with tf.device(\"CPU\"):\n",
    "    visualize_dataset(train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c05466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The most demanding task data augmentation is being done here.\n",
    "KerasCV supports bounding box augmentation with its library of data augmentation layers.\n",
    "\"\"\"\n",
    "with tf.device(\"CPU\"):\n",
    "    augmenter = keras.Sequential(\n",
    "        layers=[\n",
    "            keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),\n",
    "            #keras_cv.layers.preprocessing.Grayscale(),\n",
    "            keras_cv.layers.JitteredResize(target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xywh\"),\n",
    "            #keras_cv.layers.RandomShear(x_factor=(0.2, 0.2), y_factor=(0.2, 0.2),bounding_box_format=\"xywh\"),\n",
    "            #keras_cv.layers.Mosaic(bounding_box_format=\"xywh\")\n",
    "            #keras_cv.layers.MixUp() \n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    visualize_dataset(train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"CPU\"):\n",
    "    visualize_dataset(eval_ds,bounding_box_format=\"xywh\",value_range=(0, 255),rows=2,cols=3,# path=\"eval.png\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b5bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gray scale images\n",
    "res_gray = keras.Sequential(\n",
    "        layers=[keras_cv.layers.Resizing(640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True),\n",
    "                keras_cv.layers.preprocessing.Grayscale(output_channels=3),\n",
    "               ])\n",
    "train_ds = train_ds.map(res_gray, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "visualize_dataset(train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=3)\n",
    "eval_ds = eval_ds.map(res_gray, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "visualize_dataset(eval_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=3)\n",
    "test_ds = test_ds.map(res_gray, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "visualize_dataset(test_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.device(\"CPU\"):\n",
    "\n",
    "    inference_resizing = keras_cv.layers.Resizing(640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True)\n",
    "    train_ds = train_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    visualize_dataset(train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.device(\"CPU\"):\n",
    "\n",
    "    inference_resizing = keras_cv.layers.Resizing(640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True)\n",
    "    eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    visualize_dataset(eval_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.device(\"CPU\"):\n",
    "\n",
    "    inference_resizing = keras_cv.layers.Resizing(640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True)\n",
    "    test_ds = test_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    visualize_dataset(test_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we prepare the data to feed into our model by upacking from preprocessing dictionary.\n",
    "We need to unpack only in case of ragged tensors which needs to be converted to to dense tensors.\n",
    "\"\"\"\n",
    "\n",
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], bounding_box.to_dense(inputs[\"bounding_boxes\"], max_boxes=32)\n",
    "\n",
    "with tf.device(\"CPU\"):\n",
    "    train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    eval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c213b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the most important call is prefetch: fetch the data to create \n",
    "data in the background while current data is being processed\"\"\"\n",
    "with tf.device(\"CPU\"):\n",
    "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    #test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385941b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_metrics = keras_cv.metrics.BoxCOCOMetrics(bounding_box_format=\"xywh\", evaluate_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2fa217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics):\n",
    "    maxlen = max([len(key) for key in result.keys()])\n",
    "    print(\"Metrics:\")\n",
    "    print(\"-\" * (maxlen + 1))\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k.ljust(maxlen+1)}: {v.numpy():0.6f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe08204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing predicted boxes\n",
    "\n",
    "with tf.device(\"CPU\"):\n",
    "    visualization_ds = eval_ds.unbatch()\n",
    "    visualization_ds = visualization_ds.ragged_batch(16)\n",
    "    visualization_ds = visualization_ds.shuffle(8)\n",
    "\n",
    "class VisualizeDetections(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        visualize_detections(self.model, bounding_box_format=\"xywh\", dataset=visualization_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50047491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(model, dataset, bounding_box_format):\n",
    "    images, y_true = next(iter(dataset.take(1)))\n",
    "    y_pred = model.predict(images)\n",
    "    y_pred = bounding_box.to_ragged(y_pred)\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        scale=4,\n",
    "        rows=2,\n",
    "        cols=4,\n",
    "        show=True,\n",
    "        font_scale=0.7,\n",
    "        class_mapping=class_label_dic,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 0.005\n",
    "# including a global_clipnorm is extremely important in object detection tasks\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=base_lr, momentum=0.9, global_clipnorm=10.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ef96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_cv.models.RetinaNet.from_preset(\n",
    "    \"mobilenet_v3_large_imagenet\",\n",
    "    num_classes=len(class_label_dic),\n",
    "    bounding_box_format=\"xywh\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7123050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_checkpoint_callback = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=10),\n",
    "    keras.callbacks.EarlyStopping(patience=20),\n",
    "    keras.callbacks.TensorBoard(log_dir=\"logs\"),\n",
    "    keras.callbacks.ModelCheckpoint(os.path.join(CHECKPOINT_PATH, 'model.{epoch:02d}-{val_MaP:.2f}'), \n",
    "                                    save_best_only=True, mode = 'max',monitor='val_MaP',\n",
    "                                    save_weights_only=True)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b5f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    classification_loss=\"focal\", \n",
    "    box_loss=\"smoothl1\",\n",
    "    optimizer=optimizer,\n",
    "    metrics=[coco_metrics],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data= eval_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[VisualizeDetections(), model_checkpoint_callback],\n",
    ")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'train_history/mobilenet_v3_large_imagenet/'\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "    os.makedirs(path)\n",
    "with open(os.path.join(path,'history_wp'), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1716aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'train_history/mobilenet_v3_large_imagenet/'\n",
    "with open(os.path.join(path,'history_wp'), \"rb\") as file_pi:\n",
    "    history = pickle.load(file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019af075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot MaP curves\n",
    "\n",
    "plt.plot(history['MaP'])\n",
    "plt.plot(history['val_MaP'])\n",
    "plt.title('Mean avg Precision')\n",
    "plt.ylabel('MaP')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['MaP', 'val_MaP'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0809068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot accuracy and loss curves\n",
    "\n",
    "\n",
    "# list all data in history\n",
    "print(history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history['box_loss'])\n",
    "plt.plot(history['val_box_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['box_loss', 'val_box_loss'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history['classification_loss'])\n",
    "plt.plot(history['val_classification_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['classification_loss', 'val_classification_loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd57137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history['percent_boxes_matched_with_anchor'])\n",
    "plt.plot(history['val_percent_boxes_matched_with_anchor'])\n",
    "plt.title('percent_boxes_matched_with_anchor')\n",
    "plt.ylabel('percent_boxes_matched_with_anchor')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['percent_boxes_matched_with_anchor', 'val_percent_boxes_matched_with_anchor'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('train_history/mobilenet_v3_large_imagenet/with_sampling/no_aug/checkpoint/model.47-0.46')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9081d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Keras CV evaluation metrics\n",
    "\n",
    "#Ragged tensor needs to be converted to the dense tensor in order to work with COCO metric, \n",
    "\n",
    "coco_metrics = keras_cv.metrics.BoxCOCOMetrics(bounding_box_format=\"xywh\", evaluate_freq=1)\n",
    "\n",
    "model.compile(\n",
    "    classification_loss=\"focal\",\n",
    "    box_loss=\"smoothl1\",\n",
    "    optimizer=optimizer,\n",
    "    metrics=[coco_metrics],\n",
    "    )\n",
    "\n",
    "coco_metrics.reset_state()\n",
    "result = model.evaluate(eval_ds.take(-1), verbose=0)\n",
    "result = coco_metrics.result(force=True)\n",
    "\n",
    "print_metrics(result)\n",
    "\n",
    "\"\"\"The most common metric to evaluate performance of Object detection model is Mean average\n",
    "Precision (MaP).\n",
    "Average precision calculates area under the precision-recall curve.\n",
    "We want to increase the value of MaP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing predicted boxes\n",
    "\n",
    "visualization_ds = test_ds.unbatch()\n",
    "visualization_ds = visualization_ds.ragged_batch(BATCH_SIZE)\n",
    "#visualization_ds = visualization_ds.shuffle(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data is divided into different batches of size defined in the beginning\"\"\"\n",
    "coco_metrics = keras_cv.metrics.BoxCOCOMetrics(bounding_box_format=\"xywh\", evaluate_freq=1)\n",
    "\n",
    "eval_ub = test_ds.unbatch()\n",
    "eval_ub = eval_ub.ragged_batch(len(eval_ub))\n",
    "\n",
    "coco_metrics.reset_state()\n",
    "y_p = []\n",
    "y_t = []\n",
    "for images, y_true in tqdm.tqdm(iter(eval_ub)):\n",
    "    y_pred = model.predict(images, verbose=0)\n",
    "    coco_metrics.update_state(y_true, y_pred)\n",
    "    y_p.append(y_pred)\n",
    "    y_t.append(bounding_box.to_ragged(y_true))\n",
    "    print_metrics(coco_metrics.result(force=True))\n",
    "result = coco_metrics.result(force=True)\n",
    "print_metrics(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5cbd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n",
    "    bounding_box_format=\"xywh\",\n",
    "    from_logits=True,\n",
    "    iou_threshold=0.75,\n",
    "    confidence_threshold=0.75,\n",
    ")\n",
    "\n",
    "\n",
    "eval_ub = eval_ds.unbatch()\n",
    "eval_ub = eval_ub.ragged_batch(8)\n",
    "\n",
    "i = 0\n",
    "for images, y_true in tqdm.tqdm(iter(eval_ub)):\n",
    "    i = i+1\n",
    "    y_pred = model.predict(images)\n",
    "    y_pred = bounding_box.to_ragged(y_pred)\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "            images,\n",
    "            value_range=(0, 255),\n",
    "            bounding_box_format=\"xywh\",\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            scale=8,\n",
    "            rows=2,\n",
    "            cols=3,\n",
    "            #show=True,\n",
    "            font_scale=0.6,\n",
    "            class_mapping=class_label_dic,\n",
    "            legend=True,\n",
    "            path=os.path.join('eval_images','{}'.format(i)),\n",
    "    )\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
